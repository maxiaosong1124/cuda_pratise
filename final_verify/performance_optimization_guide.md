# ğŸš€ CUDAæ€§èƒ½ä¼˜åŒ–å®æˆ˜æŒ‡å—

å­¦å®Œ10èŠ‚è¯¾ç¨‹åï¼Œè¿™é‡Œæ˜¯è¿›é˜¶çš„æ€§èƒ½ä¼˜åŒ–å®æˆ˜å»ºè®®ï¼Œå¸®ä½ å°†ç†è®ºçŸ¥è¯†è½¬åŒ–ä¸ºå®é™…çš„ä¼˜åŒ–æŠ€èƒ½ã€‚

## ğŸ“Š åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½åˆ†æ

### 1. å»ºç«‹æ€§èƒ½åŸºçº¿

é¦–å…ˆï¼Œä¸ºæ¯ä¸ªcourseçš„ç®—æ³•å»ºç«‹æ€§èƒ½åŸºçº¿ï¼š

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•
cd /home/maxiaosong/work_space/cuda_learning/cuda_code
mkdir performance_logs

# course5 çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•
./build/matmul3 1024 1024 1024 > performance_logs/matmul_baseline.txt

# course9 attentionæ€§èƒ½æµ‹è¯•  
./build/flash_attn 512 64 8 > performance_logs/attention_baseline.txt
```

### 2. ä½¿ç”¨profilingå·¥å…·æ·±åº¦åˆ†æ

```bash
# ä½¿ç”¨Nsight Computeåˆ†æå†…å­˜è®¿é—®æ¨¡å¼
ncu --metrics smsp__inst_executed_per_warp,l1tex__t_bytes_pipe_lsu_mem_global_op_ld \
    --target-processes all ./build/matmul3 2048 2048 2048

# åˆ†æwarpæ•ˆç‡å’Œoccupancy
ncu --metrics smsp__warps_active.avg,smsp__warps_eligible.avg \
    --target-processes all ./build/reduce_v4

# å†…å­˜å¸¦å®½åˆ©ç”¨ç‡åˆ†æ
ncu --metrics dram__bytes_read.sum,dram__bytes_write.sum \
    --target-processes all ./build/transpose_bench
```

## âš¡ å…·ä½“ä¼˜åŒ–å®æˆ˜é¡¹ç›®

### é¡¹ç›®1: çŸ©é˜µä¹˜æ³•æè‡´ä¼˜åŒ– (åŸºäºcourse5)

**ç›®æ ‡**: è¾¾åˆ°cuBLAS 80%ä»¥ä¸Šçš„æ€§èƒ½

**ä¼˜åŒ–ç­–ç•¥**:
1. **åŒç¼“å†²æŠ€æœ¯**: é‡å è®¡ç®—ä¸å†…å­˜åŠ è½½
2. **å‘é‡åŒ–è®¿é—®**: ä½¿ç”¨float4è¿›è¡Œå†…å­˜è®¿é—®
3. **warp specialization**: ä¸åŒwarpå¤„ç†ä¸åŒä»»åŠ¡

```cpp
// é«˜çº§ä¼˜åŒ–ç‰ˆæœ¬æ¨¡æ¿
template<int BLOCK_M, int BLOCK_N, int BLOCK_K, int WARP_M, int WARP_N>
__global__ void optimized_sgemm_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B, 
    float* __restrict__ C,
    int M, int N, int K) {
    
    // 1. è®¡ç®—warpå’Œçº¿ç¨‹ä½ç½®
    const int warp_id = threadIdx.x / 32;
    const int lane_id = threadIdx.x % 32;
    
    // 2. ä½¿ç”¨åŒç¼“å†²shared memory
    __shared__ float A_shared[2][BLOCK_M][BLOCK_K];
    __shared__ float B_shared[2][BLOCK_K][BLOCK_N];
    
    // 3. å¯„å­˜å™¨tiling
    float C_reg[WARP_M/16][WARP_N/16] = {0.0f};
    
    // 4. ä¸»å¾ªç¯ï¼šé‡å è®¡ç®—ä¸æ•°æ®åŠ è½½
    for (int k_block = 0; k_block < K; k_block += BLOCK_K) {
        // å¼‚æ­¥åŠ è½½æ•°æ®åˆ°shared memory
        // ä½¿ç”¨å‘é‡åŒ–è®¿é—® (float4)
        
        // warp-level GEMMè®¡ç®—
        // ä½¿ç”¨tensor coreæˆ–æ‰‹å·¥ä¼˜åŒ–çš„ç‚¹ç§¯
        
        __syncthreads();
    }
    
    // 5. å†™å›ç»“æœ (å‘é‡åŒ–å†™å…¥)
}
```

**æ€§èƒ½éªŒè¯æ ‡å‡†**:
- 2048x2048çŸ©é˜µ: > 2000 GFLOPS
- å†…å­˜å¸¦å®½åˆ©ç”¨ç‡: > 80%
- ä¸cuBLASæ€§èƒ½å·®è·: < 20%

### é¡¹ç›®2: Flash Attentionæ·±åº¦ä¼˜åŒ– (åŸºäºcourse9)

**ç›®æ ‡**: å®ç°production-readyçš„Flash Attention

**ä¼˜åŒ–é‡ç‚¹**:
1. **å†…å­˜è®¿é—®ä¼˜åŒ–**: å‡å°‘HBMè®¿é—®
2. **æ•°å€¼ç¨³å®šæ€§**: åœ¨çº¿softmaxè®¡ç®—
3. **åºåˆ—é•¿åº¦é€‚é…**: æ”¯æŒä»»æ„é•¿åº¦åºåˆ—

```cpp
// Flash Attentionæ ¸å¿ƒä¼˜åŒ–æ€è·¯
__global__ void flash_attention_v2(
    const float* Q, const float* K, const float* V,
    float* O, float* L, float* M,  // L:row sum, M:row max
    int seq_len, int head_dim, int num_heads) {
    
    // 1. æ¯ä¸ªblockå¤„ç†ä¸€ä¸ªqueryåºåˆ—ä½ç½®
    const int batch_idx = blockIdx.y;
    const int head_idx = blockIdx.z;
    const int q_idx = blockIdx.x;
    
    // 2. åœ¨çº¿ç®—æ³•ç»´æŠ¤ç»Ÿè®¡é‡
    float row_max = -INFINITY;
    float row_sum = 0.0f;
    float output[HEAD_DIM] = {0.0f};
    
    // 3. åˆ†å—å¤„ç†key-value
    for (int kv_block = 0; kv_block < seq_len; kv_block += BLOCK_SIZE) {
        // è®¡ç®—attention scores
        // åœ¨çº¿æ›´æ–°maxå’Œsum
        // æ›´æ–°output
    }
    
    // 4. å†™å›æœ€ç»ˆç»“æœ
}
```

### é¡¹ç›®3: é€šç”¨Reductionä¼˜åŒ–å™¨ (åŸºäºcourse4)

**ç›®æ ‡**: è®¾è®¡è‡ªåŠ¨è°ƒä¼˜çš„reductionåº“

**ç‰¹æ€§**:
- æ”¯æŒä»»æ„reductionæ“ä½œ (sum, max, min, etc.)
- è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®
- æ”¯æŒä¸åŒæ•°æ®ç±»å‹

```cpp
// é€šç”¨reductionæ¨¡æ¿
template<typename T, typename Op, int BLOCK_SIZE>
class OptimizedReduction {
public:
    static T reduce(const T* input, int n, Op op) {
        // 1. è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜gridé…ç½®
        auto config = select_optimal_config(n);
        
        // 2. å¤šçº§reduction
        if (n > threshold) {
            return multi_stage_reduce(input, n, op);
        } else {
            return single_stage_reduce(input, n, op);
        }
    }
    
private:
    // ä¸åŒçš„reductionç­–ç•¥
    static T warp_reduce(T val, Op op);
    static T block_reduce(T val, Op op);
    static Config select_optimal_config(int n);
};
```

## ğŸ”§ ç³»ç»Ÿçº§ä¼˜åŒ–å®è·µ

### 1. å†…å­˜æ± ç®¡ç†

```cpp
class CUDAMemoryPool {
private:
    std::unordered_map<size_t, std::vector<void*>> free_blocks_;
    std::unordered_map<void*, size_t> allocated_blocks_;
    
public:
    void* allocate(size_t bytes) {
        // æŸ¥æ‰¾åˆé€‚å¤§å°çš„ç©ºé—²å—
        // å¦‚æœæ²¡æœ‰ï¼Œåˆ†é…æ–°å—
    }
    
    void deallocate(void* ptr) {
        // æ ‡è®°ä¸ºç©ºé—²ï¼Œä¸ç«‹å³é‡Šæ”¾
    }
    
    void defragment() {
        // å†…å­˜ç¢ç‰‡æ•´ç†
    }
};
```

### 2. å¼‚æ­¥æ‰§è¡Œä¼˜åŒ–

```cpp
class AsyncExecutor {
private:
    std::vector<cudaStream_t> streams_;
    std::queue<Task> task_queue_;
    
public:
    void submit_task(Task task) {
        // é€‰æ‹©è´Ÿè½½æœ€è½»çš„stream
        int stream_id = select_optimal_stream();
        task.execute_async(streams_[stream_id]);
    }
    
    void wait_all() {
        for (auto& stream : streams_) {
            cudaStreamSynchronize(stream);
        }
    }
};
```

### 3. æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

```cpp
class PerformanceMonitor {
public:
    void start_timer(const std::string& kernel_name) {
        auto& timer = timers_[kernel_name];
        cudaEventRecord(timer.start);
    }
    
    void end_timer(const std::string& kernel_name) {
        auto& timer = timers_[kernel_name];
        cudaEventRecord(timer.stop);
        cudaEventSynchronize(timer.stop);
        
        float elapsed;
        cudaEventElapsedTime(&elapsed, timer.start, timer.stop);
        update_statistics(kernel_name, elapsed);
    }
    
    void print_report() {
        // ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        for (const auto& [name, stats] : kernel_stats_) {
            std::cout << name << ": avg=" << stats.avg_time 
                      << "ms, max=" << stats.max_time << "ms\n";
        }
    }
};
```

## ğŸ¯ å®æˆ˜æŒ‘æˆ˜é¡¹ç›®

### æŒ‘æˆ˜1: æ·±åº¦å­¦ä¹ ç®—å­åº“
å®ç°åŒ…å«ä»¥ä¸‹ç®—å­çš„é«˜æ€§èƒ½åº“ï¼š
- **åŸºç¡€ç®—å­**: GEMM, Conv2D, BatchNorm, LayerNorm
- **æ¿€æ´»å‡½æ•°**: ReLU, GELU, SiLUåŠå…¶å¯¼æ•°
- **æ³¨æ„åŠ›æ¨¡å—**: Multi-Head Attention, Flash Attention
- **ä¼˜åŒ–å™¨**: Adam, AdamWçš„parameter update

**æ€§èƒ½ç›®æ ‡**:
- ä¸PyTorch/cuDNNæ€§èƒ½å·®è· < 15%
- æ”¯æŒæ··åˆç²¾åº¦ (FP16/BF16)
- å†…å­˜ä½¿ç”¨æ•ˆç‡ > 85%

### æŒ‘æˆ˜2: å›¾åƒå¤„ç†åŠ é€Ÿåº“
- **æ»¤æ³¢æ“ä½œ**: é«˜æ–¯æ»¤æ³¢, åŒè¾¹æ»¤æ³¢
- **å‡ ä½•å˜æ¢**: æ—‹è½¬, ç¼©æ”¾, é€è§†å˜æ¢
- **ç‰¹å¾æå–**: Harrisè§’ç‚¹, SIFTç‰¹å¾

### æŒ‘æˆ˜3: ç§‘å­¦è®¡ç®—åº“
- **çº¿æ€§ä»£æ•°**: ç‰¹å¾å€¼åˆ†è§£, SVD, QRåˆ†è§£
- **ä¿¡å·å¤„ç†**: FFT, å·ç§¯, ç›¸å…³æ€§è®¡ç®—
- **æ•°å€¼æ–¹æ³•**: ç¨€ç–çŸ©é˜µè¿ç®—, è¿­ä»£æ±‚è§£å™¨

## ğŸ“ˆ è¿›é˜¶å­¦ä¹ è·¯å¾„

### é˜¶æ®µ1: æ·±åŒ–åŸºç¡€ (1-2ä¸ªæœˆ)
1. **å®Œå–„profilingæŠ€èƒ½**
   - ç†Ÿç»ƒä½¿ç”¨Nsight Computeæ‰€æœ‰åŠŸèƒ½
   - å­¦ä¼šè¯»æ‡‚roofline modelåˆ†æ
   - æŒæ¡å†…å­˜è®¿é—®æ¨¡å¼åˆ†æ

2. **ç®—æ³•ä¼˜åŒ–æ¨¡å¼**
   - å­¦ä¹ å¸¸è§çš„ä¼˜åŒ–pattern
   - æŒæ¡ä¸åŒGPUæ¶æ„çš„ç‰¹ç‚¹
   - äº†è§£ç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯

### é˜¶æ®µ2: ç³»ç»Ÿçº§ä¼˜åŒ– (2-3ä¸ªæœˆ)
1. **å¤šGPUç¼–ç¨‹**
   - å­¦ä¹ NCCLé›†åˆé€šä¿¡
   - æŒæ¡æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œ
   - å®ç°é«˜æ•ˆçš„gradient allreduce

2. **å†…å­˜ç®¡ç†é«˜çº§æŠ€æœ¯**
   - Unified Memoryç¼–ç¨‹æ¨¡å‹
   - å†…å­˜é¢„å–å’Œè¿ç§»ç­–ç•¥
   - NUMAæ„ŸçŸ¥çš„å†…å­˜åˆ†é…

### é˜¶æ®µ3: äº§ä¸šçº§åº”ç”¨ (3-6ä¸ªæœˆ)
1. **æ·±åº¦å­¦ä¹ ç³»ç»Ÿ**
   - è´¡çŒ®PyTorch CUDA kernels
   - å­¦ä¹ TensorRTä¼˜åŒ–æŠ€æœ¯
   - ç ”ç©¶Triton DSLç¼–ç¨‹

2. **HPCåº”ç”¨å¼€å‘**
   - ç§‘å­¦è®¡ç®—ç®—æ³•GPUåŒ–
   - å¤§è§„æ¨¡å¹¶è¡Œç¨‹åºè®¾è®¡
   - æ€§èƒ½è°ƒä¼˜å’Œæ‰©å±•æ€§åˆ†æ

## ğŸ† æˆæœå±•ç¤ºå»ºè®®

### åœ¨çº¿ç«æŠ€å¹³å° (æ¨è)
1. **[LeetGPU.com](https://leetgpu.com/challenges)** - ä¸“ä¸šGPUç®—å­ä¼˜åŒ–æŒ‘æˆ˜
   - çœŸå®å·¥ä¸šåœºæ™¯çš„ç®—å­ä¼˜åŒ–é¢˜ç›®
   - åŸºäºæ€§èƒ½æ’è¡Œæ¦œçš„ç«äº‰æœºåˆ¶
   - æ¶µç›–å„ç§æ·±åº¦å­¦ä¹ å’ŒHPCç®—å­
   - æä¾›benchmarkå’Œæ€§èƒ½å¯¹æ¯”

2. **ä¼ ç»Ÿå±•ç¤ºæ–¹å¼**
   - **GitHubé¡¹ç›®**: åˆ›å»ºé«˜è´¨é‡çš„CUDAé¡¹ç›®ä»£ç åº“
   - **æŠ€æœ¯åšå®¢**: åˆ†äº«ä¼˜åŒ–ç»éªŒå’Œæ€§èƒ½åˆ†æ
   - **å¼€æºè´¡çŒ®**: å‚ä¸çŸ¥åé¡¹ç›®çš„GPUä¼˜åŒ–å·¥ä½œ
   - **ç«èµ›å‚ä¸**: å‚åŠ GPUç¼–ç¨‹ç«èµ›å’Œhackathon
   - **æŠ€æœ¯æ¼”è®²**: åœ¨ä¼šè®®æˆ–meetupåˆ†äº«ç»éªŒ

## ğŸ”— æ¨èèµ„æº

### å®˜æ–¹æ–‡æ¡£
- CUDA C++ Programming Guide
- CUDA Best Practices Guide  
- Nsight Compute User Guide

### ä¼˜ç§€é¡¹ç›®å­¦ä¹ 
- [cuDNNæºç åˆ†æ](https://github.com/NVIDIA/cudnn)
- [PyTorch CUDA kernels](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/cuda)
- [cutlassé«˜æ€§èƒ½GEMM](https://github.com/NVIDIA/cutlass)

### å­¦æœ¯èµ„æº
- GPUæ¶æ„ç›¸å…³è®ºæ–‡
- é«˜æ€§èƒ½è®¡ç®—ä¼šè®® (SC, PPoPP, HPCA)
- æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¼šè®® (MLSys, EuroSys)

---

è®°ä½ï¼š**çœŸæ­£çš„å­¦ä¹ æˆæœä¸åœ¨äºé€šè¿‡äº†å¤šå°‘æµ‹è¯•ï¼Œè€Œåœ¨äºèƒ½å¦ç‹¬ç«‹è§£å†³å®é™…çš„æ€§èƒ½é—®é¢˜ï¼Œå¹¶åˆ›é€ å‡ºæœ‰ä»·å€¼çš„GPUåŠ é€Ÿè§£å†³æ–¹æ¡ˆï¼** ğŸš€