# ğŸ† LeetGPUé£æ ¼CUDAæŒ‘æˆ˜é¢˜é›†

åŸºäº10èŠ‚courseå­¦ä¹ å†…å®¹ï¼Œè®¾è®¡çš„ç«æŠ€å¼ç®—å­ä¼˜åŒ–æŒ‘æˆ˜ï¼Œæ¨¡æ‹ŸLeetGPU.comçš„è¯„ä¼°æ–¹å¼ã€‚

## ğŸ“Š æŒ‘æˆ˜è¯„ä¼°ä½“ç³»

### è¯„åˆ†æ ‡å‡†
- **æ­£ç¡®æ€§** (40%): ç®—æ³•ç»“æœå¿…é¡»å®Œå…¨æ­£ç¡®
- **æ€§èƒ½** (40%): åŸºäºGFLOPSã€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡æ’å
- **ä»£ç è´¨é‡** (20%): å¯è¯»æ€§ã€é”™è¯¯å¤„ç†ã€è¾¹ç•Œæƒ…å†µ

### éš¾åº¦åˆ†çº§
- ğŸŸ¢ **Easy**: åŸºç¡€ç®—å­å®ç°
- ğŸŸ¡ **Medium**: æ€§èƒ½ä¼˜åŒ–æŒ‘æˆ˜  
- ğŸ”´ **Hard**: å¤æ‚ç®—æ³•å’Œæè‡´ä¼˜åŒ–
- ğŸŸ£ **Expert**: å·¥ä¸šçº§éš¾é¢˜

---

## ğŸŸ¢ Easyçº§åˆ«æŒ‘æˆ˜

### Challenge 1: Vector Operations Suite
**é¢˜ç›®**: å®ç°é«˜æ•ˆçš„å‘é‡è¿ç®—ç®—å­åº“

```cpp
// è¦æ±‚å®ç°ä»¥ä¸‹æ‰€æœ‰æ“ä½œï¼Œæ”¯æŒä»»æ„é•¿åº¦å‘é‡
__global__ void vector_add(const float* a, const float* b, float* c, int n);
__global__ void vector_mul(const float* a, const float* b, float* c, int n); 
__global__ void vector_dot_product(const float* a, const float* b, float* result, int n);
__global__ void vector_norm_l2(const float* a, float* result, int n);
__global__ void vector_scale(const float* a, float scalar, float* b, int n);
```

**æ€§èƒ½ç›®æ ‡**:
- å‘é‡é•¿åº¦ 1M: > 200 GB/s å†…å­˜å¸¦å®½
- æ”¯æŒéå¯¹é½å†…å­˜è®¿é—®
- å¤„ç†ä»»æ„å¤§å°è¾“å…¥

### Challenge 2: Matrix Transpose Optimizer
**é¢˜ç›®**: å®ç°å„ç§çŸ©é˜µè½¬ç½®ç®—æ³•å¹¶ä¼˜åŒ–åˆ°æè‡´

```cpp
// å®ç°å¤šç§è½¬ç½®ç­–ç•¥
__global__ void naive_transpose(const float* input, float* output, int rows, int cols);
__global__ void shared_memory_transpose(const float* input, float* output, int rows, int cols);
__global__ void bank_conflict_free_transpose(const float* input, float* output, int rows, int cols);
__global__ void vectorized_transpose(const float* input, float* output, int rows, int cols);
```

**Leaderboardç›®æ ‡**:
- 2048x2048çŸ©é˜µ: > 400 GB/s
- éæ–¹é˜µä¼˜åŒ–: 1024x4096 > 350 GB/s
- Bank conflict < 1%

---

## ğŸŸ¡ Mediumçº§åˆ«æŒ‘æˆ˜

### Challenge 3: Reduction Tournament  
**é¢˜ç›®**: å®ç°æœ€å¿«çš„å½’çº¦ç®—æ³•

```cpp
// æ”¯æŒä¸åŒçš„å½’çº¦æ“ä½œ
template<typename T, typename Op>
__global__ void ultimate_reduce(const T* input, T* output, int n, Op operation);

// æ”¯æŒçš„æ“ä½œ: sum, max, min, product, logical_and, logical_or
```

**ç«æŠ€è§„åˆ™**:
- è¾“å…¥å¤§å°: 1M - 1B elements
- ä¸CUBåº“æ€§èƒ½å¯¹æ¯”
- ç›®æ ‡: è¾¾åˆ°CUB 90%ä»¥ä¸Šæ€§èƒ½

### Challenge 4: GEMM Speed Run
**é¢˜ç›®**: çŸ©é˜µä¹˜æ³•æé€ŸæŒ‘æˆ˜

```cpp
// å®ç°å•ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼Œè¿½æ±‚æè‡´æ€§èƒ½
__global__ void speed_sgemm(const float* A, const float* B, float* C, 
                           int M, int N, int K, float alpha, float beta);
```

**Ranking System**:
- Square Matrix Benchmark: 512, 1024, 2048, 4096
- Rectangular Matrix Challenge: Mâ‰ Nâ‰ K combinations
- ç›®æ ‡: è¾¾åˆ°cuBLAS 80%æ€§èƒ½
- Bonus: æ”¯æŒbatched operations

### Challenge 5: Convolution Master
**é¢˜ç›®**: 2Då·ç§¯ç®—å­ä¼˜åŒ–å¤§å¸ˆèµ›

```cpp
// å®ç°é«˜æ•ˆçš„2Då·ç§¯
__global__ void optimized_conv2d(
    const float* input,    // [N, H, W, C]
    const float* kernel,   // [KH, KW, C, OC] 
    float* output,         // [N, OH, OW, OC]
    ConvParams params
);
```

**æµ‹è¯•åœºæ™¯**:
- å…¸å‹CNNå±‚: 224x224 input, 3x3/5x5/7x7 kernels
- ä¸åŒstrideå’Œpaddingç»„åˆ
- ä¸cuDNNæ€§èƒ½å¯¹æ¯”

---

## ğŸ”´ Hardçº§åˆ«æŒ‘æˆ˜

### Challenge 6: Flash Attention Implementation
**é¢˜ç›®**: ä»é›¶å®ç°Flash Attention

```cpp
// å†…å­˜é«˜æ•ˆçš„attentionè®¡ç®—
__global__ void flash_attention(
    const float* Q, const float* K, const float* V,
    float* output,
    int batch_size, int num_heads, int seq_len, int head_dim,
    float scale
);
```

**æŠ€æœ¯è¦æ±‚**:
- O(N)å†…å­˜å¤æ‚åº¦ (è€ŒéO(NÂ²))
- æ•°å€¼ç¨³å®šçš„åœ¨çº¿softmax
- æ”¯æŒå› æœmaskå’Œattention mask
- æ€§èƒ½ç›®æ ‡: æ¯”naive attentionå¿«2xä»¥ä¸Š

### Challenge 7: Sparse Matrix Operations
**é¢˜ç›®**: ç¨€ç–çŸ©é˜µç®—å­ä¼˜åŒ–

```cpp
// CSRæ ¼å¼ç¨€ç–çŸ©é˜µä¹˜æ³•
__global__ void sparse_sgemm_csr(
    const float* values, const int* col_indices, const int* row_ptr,
    const float* dense_matrix, float* result,
    int M, int N, int K, int nnz
);

// ç¨€ç–çŸ©é˜µå‘é‡ä¹˜æ³•
__global__ void spmv_csr(
    const float* values, const int* col_indices, const int* row_ptr,
    const float* vector, float* result, int M, int nnz
);
```

**Benchmarkæ•°æ®é›†**:
- çœŸå®ä¸–ç•Œç¨€ç–çŸ©é˜µ (ä»SuiteSparseè·å–)
- ä¸åŒç¨€ç–åº¦: 0.1%, 1%, 10%
- ä¸cuSPARSEæ€§èƒ½å¯¹æ¯”

### Challenge 8: Fused Kernel Master
**é¢˜ç›®**: èåˆç®—å­è®¾è®¡å¤§å¸ˆ

```cpp
// è®¾è®¡èåˆçš„LayerNorm + GELU + Linear
__global__ void fused_layernorm_gelu_linear(
    const float* input,           // [batch, seq_len, hidden_dim]
    const float* ln_weight,       // [hidden_dim]
    const float* ln_bias,         // [hidden_dim] 
    const float* linear_weight,   // [hidden_dim, output_dim]
    const float* linear_bias,     // [output_dim]
    float* output,                // [batch, seq_len, output_dim]
    LayerNormParams ln_params,
    LinearParams linear_params
);
```

**ä¼˜åŒ–ç›®æ ‡**:
- æœ€å°åŒ–å†…å­˜è®¿é—®æ¬¡æ•°
- æœ€å¤§åŒ–ç®—æœ¯å¼ºåº¦
- ä¸åˆ†ç¦»å®ç°çš„æ€§èƒ½å¯¹æ¯”

---

## ğŸŸ£ Expertçº§åˆ«æŒ‘æˆ˜

### Challenge 9: Auto-Tuning GEMM
**é¢˜ç›®**: è‡ªåŠ¨è°ƒä¼˜çš„çŸ©é˜µä¹˜æ³•åº“

```cpp
// å®ç°èƒ½è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®çš„GEMM
class AutoTunedGEMM {
public:
    // è‡ªåŠ¨benchmarkå¹¶é€‰æ‹©æœ€ä¼˜kernel
    void auto_tune(int M, int N, int K);
    
    // æ‰§è¡Œä¼˜åŒ–åçš„GEMM
    void execute(const float* A, const float* B, float* C, 
                int M, int N, int K, float alpha, float beta);
                
private:
    // å¤šç§ä¸åŒçš„kernelå®ç°
    std::vector<GemmKernel> kernel_variants_;
    
    // æ€§èƒ½æ•°æ®åº“
    PerformanceDB perf_db_;
};
```

**æŠ€æœ¯æŒ‘æˆ˜**:
- æ”¯æŒ20+ç§ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
- è¿è¡Œæ—¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®
- æ„å»ºæ€§èƒ½é¢„æµ‹æ¨¡å‹
- æ”¯æŒå¤šGPUæ¶æ„é€‚é…

### Challenge 10: Distributed GPU Computing
**é¢˜ç›®**: å¤šGPUåä½œè®¡ç®—æ¡†æ¶

```cpp
// è·¨å¤šä¸ªGPUçš„å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•
class MultiGPUGemm {
public:
    void distributed_gemm(
        const float* A, const float* B, float* C,
        int M, int N, int K, int num_gpus
    );
    
private:
    void partition_work(int M, int N, int K, int num_gpus);
    void execute_distributed();
    void allreduce_results();
};
```

**ç³»ç»Ÿè¦æ±‚**:
- æ”¯æŒ2-8ä¸ªGPUåä½œ
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡
- é€šä¿¡å¼€é”€æœ€å°åŒ–
- æ‰©å±•æ€§åˆ†ææŠ¥å‘Š

---

## ğŸ¯ å®æˆ˜æ¯”èµ›æ¨¡æ‹Ÿ

### Weekly Challenge Format

æ¯å‘¨å‘å¸ƒæ–°æŒ‘æˆ˜ï¼Œæ¨¡æ‹ŸçœŸå®ç«èµ›ç¯å¢ƒï¼š

```bash
# æ¯”èµ›æ¨¡æ‹Ÿè„šæœ¬
#!/bin/bash

echo "ğŸ CUDAç®—å­ä¼˜åŒ–æŒ‘æˆ˜èµ› Week $1"
echo "é¢˜ç›®: $2"
echo "æ—¶é—´é™åˆ¶: 48å°æ—¶"
echo "============================================"

# 1. ä¸‹è½½æµ‹è¯•æ•°æ®å’Œbaseline
wget https://challenge-data/week$1/testdata.tar.gz
wget https://challenge-data/week$1/baseline_results.json

# 2. ç¼–è¯‘æµ‹è¯•
nvcc -O3 -arch=native solution.cu -o solution

# 3. æ€§èƒ½æµ‹è¯•
./solution < testdata/input1.txt > output1.txt
./benchmark_tool solution

# 4. æ­£ç¡®æ€§éªŒè¯
python verify_correctness.py output1.txt expected1.txt

# 5. æ’è¡Œæ¦œæäº¤
curl -X POST https://leaderboard/submit \
     -F "solution=@solution.cu" \
     -F "performance=@benchmark_results.json"
```

### Leaderboard System

```python
# æ’è¡Œæ¦œè®¡ç®—é€»è¾‘
class LeaderboardCalculator:
    def calculate_score(self, submission):
        # 1. æ­£ç¡®æ€§æ£€éªŒ (å¿…é¡»100%æ­£ç¡®)
        correctness = verify_correctness(submission.output)
        if not correctness:
            return 0
            
        # 2. æ€§èƒ½å¾—åˆ† (ä¸baselineå¯¹æ¯”)
        perf_ratio = submission.performance / baseline_performance
        perf_score = min(100, perf_ratio * 100)
        
        # 3. ä»£ç è´¨é‡å¾—åˆ†
        quality_score = analyze_code_quality(submission.code)
        
        # 4. ç»¼åˆå¾—åˆ†
        final_score = (
            correctness * 0.4 +      # 40%
            perf_score * 0.4 +       # 40% 
            quality_score * 0.2      # 20%
        )
        
        return final_score
```

### å­¦ä¹ è¿›åº¦è¿½è¸ª

```cpp
// ä¸ªäººå­¦ä¹ æ¡£æ¡ˆ
struct LearningProfile {
    int total_challenges_attempted = 0;
    int challenges_solved = 0;
    float average_performance_ratio = 0.0f;
    
    std::map<std::string, float> skill_ratings = {
        {"memory_optimization", 0.0f},
        {"algorithm_design", 0.0f},
        {"parallel_thinking", 0.0f},
        {"debugging_skills", 0.0f}
    };
    
    std::vector<Achievement> unlocked_achievements;
};
```

---

## ğŸ“ˆ ä¸è¯¾ç¨‹å†…å®¹çš„å¯¹åº”å…³ç³»

| Challenge | å¯¹åº”Course | æ ¸å¿ƒæŠ€èƒ½ |
|-----------|------------|----------|
| Vector Ops | Course 1-2 | åŸºç¡€kernelç¼–å†™ |
| Transpose | Course 7 | å†…å­˜è®¿é—®ä¼˜åŒ– |
| Reduction | Course 4 | warp primitives |
| GEMM | Course 5 | å…±äº«å†…å­˜tiling |
| Convolution | Course 5æ‰©å±• | å¤æ‚ç®—æ³•è®¾è®¡ |
| Flash Attention | Course 9 | å†…å­˜é«˜æ•ˆç®—æ³• |
| Sparse Ops | ç»¼åˆåº”ç”¨ | ä¸è§„åˆ™è®¿é—®ä¼˜åŒ– |
| Fused Kernels | Course 8æ‰©å±• | èåˆä¼˜åŒ–ç­–ç•¥ |
| Auto-tuning | ç³»ç»Ÿçº§ä¼˜åŒ– | è‡ªé€‚åº”ç®—æ³• |
| Multi-GPU | åˆ†å¸ƒå¼è®¡ç®— | ç³»ç»Ÿæ¶æ„è®¾è®¡ |

---

## ğŸš€ å¼€å§‹æŒ‘æˆ˜ï¼

```bash
# åˆ›å»ºæŒ‘æˆ˜ç¯å¢ƒ
cd /home/maxiaosong/work_space/cuda_learning/cuda_code/course10
mkdir leetgpu_challenges
cd leetgpu_challenges

# å¼€å§‹ç¬¬ä¸€ä¸ªæŒ‘æˆ˜
git clone https://github.com/your-repo/cuda-challenges.git
cd cuda-challenges/challenge-001-vector-ops

# æŸ¥çœ‹é¢˜ç›®
cat README.md
cat test_cases.json

# å¼€å§‹ç¼–ç ï¼
cp template.cu solution.cu
# ç¼–è¾‘ solution.cu å®ç°ç®—æ³•

# æœ¬åœ°æµ‹è¯•
make test
make benchmark

# æŸ¥çœ‹æ’è¡Œæ¦œ
make leaderboard
```

**è¿™ç§ç«æŠ€å¼æ£€éªŒæ–¹å¼çš„ä¼˜åŠ¿**:
1. **å®æˆ˜å¯¼å‘**: çœŸå®çš„æ€§èƒ½ä¼˜åŒ–åœºæ™¯
2. **æŒç»­æ¿€åŠ±**: æ’è¡Œæ¦œå’Œç«äº‰æœºåˆ¶
3. **å…¨é¢è¯„ä¼°**: ä¸ä»…è€ƒè™‘æ­£ç¡®æ€§ï¼Œæ›´é‡è§†æ€§èƒ½
4. **ç¤¾åŒºå­¦ä¹ **: å¯ä»¥å­¦ä¹ ä»–äººçš„ä¼˜åŒ–æŠ€å·§

LeetGPU.comè¿™æ ·çš„å¹³å°ç¡®å®æ˜¯æ£€éªŒCUDAå­¦ä¹ æˆæœçš„**ç»ä½³æ–¹å¼**ï¼ğŸ†