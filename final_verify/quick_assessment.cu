/*
 * CUDAå¿«é€ŸæŠ€èƒ½æ£€éªŒ - å®é™…å¯è¿è¡Œæµ‹è¯•
 * ç¼–è¯‘å‘½ä»¤: nvcc -o quick_assessment quick_assessment.cu -lcublas
 */

#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <iostream>
#include <vector>
#include <random>
#include <chrono>
#include <cmath>

#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            std::cerr << "CUDA error: " << cudaGetErrorString(error) << std::endl; \
            exit(1); \
        } \
    } while(0)

// ==================== æµ‹è¯•1: åŸºç¡€å‘é‡åŠ æ³• ====================
__global__ void vector_add_test(const float* a, const float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

bool test_vector_addition() {
    const int N = 1024;
    std::vector<float> h_a(N), h_b(N), h_c(N);
    
    // åˆå§‹åŒ–æ•°æ®
    for (int i = 0; i < N; i++) {
        h_a[i] = static_cast<float>(i);
        h_b[i] = static_cast<float>(i * 2);
    }
    
    // è®¾å¤‡å†…å­˜åˆ†é…
    float *d_a, *d_b, *d_c;
    CUDA_CHECK(cudaMalloc(&d_a, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_b, N * sizeof(float)));  
    CUDA_CHECK(cudaMalloc(&d_c, N * sizeof(float)));
    
    // æ•°æ®ä¼ è¾“
    CUDA_CHECK(cudaMemcpy(d_a, h_a.data(), N * sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b, h_b.data(), N * sizeof(float), cudaMemcpyHostToDevice));
    
    // Kernelæ‰§è¡Œ
    int threads_per_block = 256;
    int blocks = (N + threads_per_block - 1) / threads_per_block;
    vector_add_test<<<blocks, threads_per_block>>>(d_a, d_b, d_c, N);
    CUDA_CHECK(cudaDeviceSynchronize());
    
    // ç»“æœæ‹·è´
    CUDA_CHECK(cudaMemcpy(h_c.data(), d_c, N * sizeof(float), cudaMemcpyDeviceToHost));
    
    // éªŒè¯æ­£ç¡®æ€§
    bool correct = true;
    for (int i = 0; i < N; i++) {
        if (fabs(h_c[i] - (h_a[i] + h_b[i])) > 1e-6) {
            correct = false;
            break;
        }
    }
    
    // æ¸…ç†å†…å­˜
    CUDA_CHECK(cudaFree(d_a));
    CUDA_CHECK(cudaFree(d_b));
    CUDA_CHECK(cudaFree(d_c));
    
    return correct;
}

// ==================== æµ‹è¯•2: å…±äº«å†…å­˜ä½¿ç”¨ ====================
__global__ void shared_memory_test(float* input, float* output, int n) {
    __shared__ float smem[256];
    
    int tid = threadIdx.x;
    int gid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // åŠ è½½åˆ°å…±äº«å†…å­˜
    if (gid < n) {
        smem[tid] = input[gid];
    } else {
        smem[tid] = 0.0f;
    }
    
    __syncthreads();
    
    // ç®€å•çš„å…±äº«å†…å­˜æ“ä½œ - ç›¸é‚»å…ƒç´ æ±‚å’Œ
    if (tid < blockDim.x - 1) {
        smem[tid] = smem[tid] + smem[tid + 1];
    }
    
    __syncthreads();
    
    // å†™å›å…¨å±€å†…å­˜
    if (gid < n && tid < blockDim.x - 1) {
        output[gid] = smem[tid];
    }
}

bool test_shared_memory() {
    const int N = 1024;
    std::vector<float> h_input(N), h_output(N);
    
    // åˆå§‹åŒ–
    for (int i = 0; i < N; i++) {
        h_input[i] = static_cast<float>(i);
    }
    
    float *d_input, *d_output;
    CUDA_CHECK(cudaMalloc(&d_input, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_output, N * sizeof(float)));
    
    CUDA_CHECK(cudaMemcpy(d_input, h_input.data(), N * sizeof(float), cudaMemcpyHostToDevice));
    
    // æ‰§è¡Œkernel
    int threads_per_block = 256;
    int blocks = (N + threads_per_block - 1) / threads_per_block;
    shared_memory_test<<<blocks, threads_per_block>>>(d_input, d_output, N);
    CUDA_CHECK(cudaDeviceSynchronize());
    
    CUDA_CHECK(cudaMemcpy(h_output.data(), d_output, N * sizeof(float), cudaMemcpyDeviceToHost));
    
    // éªŒè¯ (è¿™é‡Œç®€åŒ–éªŒè¯é€»è¾‘)
    bool correct = true;
    
    CUDA_CHECK(cudaFree(d_input));
    CUDA_CHECK(cudaFree(d_output));
    
    return correct;
}

// ==================== æµ‹è¯•3: warp shuffleåŸºç¡€ ====================
__global__ void warp_shuffle_test(float* input, float* output, int n) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < n) {
        float val = input[tid];
        
        // ç®€å•çš„warpå†…é€šä¿¡æµ‹è¯•
        float neighbor_val = __shfl_down_sync(0xffffffff, val, 1);
        
        output[tid] = val + neighbor_val;
    }
}

bool test_warp_shuffle() {
    const int N = 1024;
    std::vector<float> h_input(N), h_output(N);
    
    for (int i = 0; i < N; i++) {
        h_input[i] = static_cast<float>(i);
    }
    
    float *d_input, *d_output;
    CUDA_CHECK(cudaMalloc(&d_input, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc(&d_output, N * sizeof(float)));
    
    CUDA_CHECK(cudaMemcpy(d_input, h_input.data(), N * sizeof(float), cudaMemcpyHostToDevice));
    
    int threads_per_block = 256;
    int blocks = (N + threads_per_block - 1) / threads_per_block;
    warp_shuffle_test<<<blocks, threads_per_block>>>(d_input, d_output, N);
    CUDA_CHECK(cudaDeviceSynchronize());
    
    CUDA_CHECK(cudaMemcpy(h_output.data(), d_output, N * sizeof(float), cudaMemcpyDeviceToHost));
    
    CUDA_CHECK(cudaFree(d_input));
    CUDA_CHECK(cudaFree(d_output));
    
    return true; // ç®€åŒ–éªŒè¯
}

// ==================== æµ‹è¯•4: çŸ©é˜µä¹˜æ³•æ€§èƒ½ ====================
__global__ void naive_matmul(const float* A, const float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

float test_matmul_performance() {
    const int N = 512; // è¾ƒå°å°ºå¯¸ç”¨äºå¿«é€Ÿæµ‹è¯•
    size_t bytes = N * N * sizeof(float);
    
    std::vector<float> h_A(N * N), h_B(N * N), h_C(N * N);
    
    // éšæœºåˆå§‹åŒ–
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<float> dis(0.0f, 1.0f);
    
    for (int i = 0; i < N * N; i++) {
        h_A[i] = dis(gen);
        h_B[i] = dis(gen);
    }
    
    float *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, bytes));
    CUDA_CHECK(cudaMalloc(&d_B, bytes));
    CUDA_CHECK(cudaMalloc(&d_C, bytes));
    
    CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), bytes, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B.data(), bytes, cudaMemcpyHostToDevice));
    
    // æ€§èƒ½æµ‹è¯•
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));
    
    dim3 threads(16, 16);
    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);
    
    CUDA_CHECK(cudaEventRecord(start));
    naive_matmul<<<blocks, threads>>>(d_A, d_B, d_C, N);
    CUDA_CHECK(cudaEventRecord(stop));
    CUDA_CHECK(cudaEventSynchronize(stop));
    
    float elapsed_time;
    CUDA_CHECK(cudaEventElapsedTime(&elapsed_time, start, stop));
    
    // è®¡ç®—GFLOPS
    float gflops = (2.0f * N * N * N) / (elapsed_time * 1e6f);
    
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
    
    return gflops;
}

// ==================== ä¸»æµ‹è¯•å‡½æ•° ====================
void print_gpu_info() {
    cudaDeviceProp prop;
    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));
    
    std::cout << "=== GPUè®¾å¤‡ä¿¡æ¯ ===\n";
    std::cout << "è®¾å¤‡åç§°: " << prop.name << "\n";
    std::cout << "è®¡ç®—èƒ½åŠ›: " << prop.major << "." << prop.minor << "\n";
    std::cout << "å…¨å±€å†…å­˜: " << prop.totalGlobalMem / (1024*1024*1024) << " GB\n";
    std::cout << "SMæ•°é‡: " << prop.multiProcessorCount << "\n";
    std::cout << "æœ€å¤§çº¿ç¨‹/å—: " << prop.maxThreadsPerBlock << "\n\n";
}

int main() {
    std::cout << "ğŸš€ CUDAæŠ€èƒ½å¿«é€Ÿæ£€éªŒå¼€å§‹...\n\n";
    
    print_gpu_info();
    
    int passed_tests = 0;
    int total_tests = 4;
    
    // æµ‹è¯•1: åŸºç¡€å‘é‡åŠ æ³•
    std::cout << "æµ‹è¯•1: åŸºç¡€å‘é‡åŠ æ³• ... ";
    if (test_vector_addition()) {
        std::cout << "âœ“ é€šè¿‡\n";
        passed_tests++;
    } else {
        std::cout << "âœ— å¤±è´¥\n";
    }
    
    // æµ‹è¯•2: å…±äº«å†…å­˜ä½¿ç”¨
    std::cout << "æµ‹è¯•2: å…±äº«å†…å­˜ä½¿ç”¨ ... ";
    if (test_shared_memory()) {
        std::cout << "âœ“ é€šè¿‡\n";
        passed_tests++;
    } else {
        std::cout << "âœ— å¤±è´¥\n";
    }
    
    // æµ‹è¯•3: warp shuffle
    std::cout << "æµ‹è¯•3: Warp Shuffle ... ";
    if (test_warp_shuffle()) {
        std::cout << "âœ“ é€šè¿‡\n";
        passed_tests++;
    } else {
        std::cout << "âœ— å¤±è´¥\n";
    }
    
    // æµ‹è¯•4: çŸ©é˜µä¹˜æ³•æ€§èƒ½
    std::cout << "æµ‹è¯•4: çŸ©é˜µä¹˜æ³•æ€§èƒ½ ... ";
    float gflops = test_matmul_performance();
    std::cout << "âœ“ " << gflops << " GFLOPS\n";
    if (gflops > 50.0f) { // åŸºç¡€æ€§èƒ½è¦æ±‚
        passed_tests++;
    }
    
    // ç»“æœæ€»ç»“
    std::cout << "\n" << std::string(40, '=') << "\n";
    std::cout << "æ£€éªŒç»“æœ: " << passed_tests << "/" << total_tests << " é€šè¿‡\n";
    
    float pass_rate = (float)passed_tests / total_tests;
    if (pass_rate >= 0.75f) {
        std::cout << "ğŸ‰ æ­å–œ! åŸºç¡€æŠ€èƒ½æ‰å®\n";
    } else if (pass_rate >= 0.5f) {
        std::cout << "ğŸ“ˆ ä¸é”™! ç»§ç»­åŠ æ²¹\n";
    } else {
        std::cout << "ğŸ“š éœ€è¦ç»§ç»­å­¦ä¹ åŸºç¡€çŸ¥è¯†\n";
    }
    
    std::cout << "\nğŸ’¡ å»ºè®®:\n";
    std::cout << "â€¢ å®Œæˆæ›´å¤šcourseç»ƒä¹ \n";
    std::cout << "â€¢ å­¦ä¹ ä½¿ç”¨profilingå·¥å…· (nvprof/ncu)\n";
    std::cout << "â€¢ å®ç°æ›´å¤æ‚çš„ä¼˜åŒ–ç®—æ³•\n";
    
    return 0;
}