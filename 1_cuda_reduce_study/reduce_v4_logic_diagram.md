# CUDA Reduce V4 - Add During Load 计算逻辑图

## 概述
V4版本的主要优化是在数据加载阶段就进行第一次求和操作，每个block处理2倍的数据量，从而减少block的数量。

## 数据分布和线程映射

```
全局内存数据分布 (N = 32M, THREAD_PER_BLOCK = 256):
┌─────────────────────────────────────────────────────────────────┐
│ Block 0 处理的数据 (512个元素)                                      │
├─────────────────────────────────────────────────────────────────┤
│ input[0] input[1] ... input[255] │ input[256] input[257] ... input[511] │
│        前256个元素                │         后256个元素                   │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│ Block 1 处理的数据 (512个元素)                                      │
├─────────────────────────────────────────────────────────────────┤
│ input[512] input[513] ... input[767] │ input[768] input[769] ... input[1023] │
│        前256个元素                    │         后256个元素                    │
└─────────────────────────────────────────────────────────────────┘

总block数量 = N / (THREAD_PER_BLOCK * 2) = 32M / 512 = 65536
```

## 线程索引计算

```
对于Block i中的Thread j:
┌─────────────────────────────────────────────────────────────────┐
│ tid = threadIdx.x + 2 * blockDim.x * blockIdx.x                │
│                                                                 │
│ 例如: Block 0, Thread 0: tid = 0 + 2 * 256 * 0 = 0            │
│       Block 0, Thread 1: tid = 1 + 2 * 256 * 0 = 1            │
│       Block 1, Thread 0: tid = 0 + 2 * 256 * 1 = 512          │
│       Block 1, Thread 1: tid = 1 + 2 * 256 * 1 = 513          │
└─────────────────────────────────────────────────────────────────┘
```

## Phase 1: 数据加载 + 第一次求和

```
每个线程加载2个数据并求和:

Block 0 示例 (THREAD_PER_BLOCK = 256):
┌──────────────────────────────────────────────────────────────────┐
│ Thread 0: shared[0] = d_input[0] + d_input[256]                  │
│ Thread 1: shared[1] = d_input[1] + d_input[257]                  │
│ Thread 2: shared[2] = d_input[2] + d_input[258]                  │
│ ...                                                              │
│ Thread 255: shared[255] = d_input[255] + d_input[511]            │
└──────────────────────────────────────────────────────────────────┘

共享内存状态 (256个元素):
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│  S0 │  S1 │  S2 │  S3 │ ... │S252 │S253 │S254 │S255
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
每个Si = input[i] + input[i+256]
```

## Phase 2: 共享内存归约过程

### 迭代1: i = 128 (blockDim.x / 2)
```
活跃线程: 0-127 (threadIdx.x < 128)

┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│ T0  │ T1  │ T2  │ T3  │ ... │T126 │T127 │ 空闲 │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
  ↓     ↓     ↓     ↓           ↓     ↓
  +=    +=    +=    +=          +=    +=
  ↓     ↓     ↓     ↓           ↓     ↓
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│T128 │T129 │T130 │T131 │ ... │T254 │T255 │     │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘

操作: shared[0] += shared[128], shared[1] += shared[129], ...
结果: 前128个位置包含4个原始元素的和
```

### 迭代2: i = 64
```
活跃线程: 0-63

┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│ T0  │ T1  │ T2  │ T3  │ ... │ T62 │ T63 │ 空闲 │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
  ↓     ↓     ↓     ↓           ↓     ↓
  +=    +=    +=    +=          +=    +=
  ↓     ↓     ↓     ↓           ↓     ↓
┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
│ T64 │ T65 │ T66 │ T67 │ ... │T126 │T127 │     │
└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘

操作: shared[0] += shared[64], shared[1] += shared[65], ...
结果: 前64个位置包含8个原始元素的和
```

### 迭代3: i = 32
```
活跃线程: 0-31
操作: shared[0] += shared[32], shared[1] += shared[33], ...
结果: 前32个位置包含16个原始元素的和
```

### 迭代4: i = 16
```
活跃线程: 0-15
操作: shared[0] += shared[16], shared[1] += shared[17], ...
结果: 前16个位置包含32个原始元素的和
```

### 迭代5: i = 8
```
活跃线程: 0-7
操作: shared[0] += shared[8], shared[1] += shared[9], ...
结果: 前8个位置包含64个原始元素的和
```

### 迭代6: i = 4
```
活跃线程: 0-3
操作: shared[0] += shared[4], shared[1] += shared[5], ...
结果: 前4个位置包含128个原始元素的和
```

### 迭代7: i = 2
```
活跃线程: 0-1
操作: shared[0] += shared[2], shared[1] += shared[3]
结果: 前2个位置包含256个原始元素的和
```

### 迭代8: i = 1
```
活跃线程: 0
操作: shared[0] += shared[1]
结果: shared[0] 包含512个原始元素的和
```

## 最终结果输出

```
只有Thread 0执行:
d_output[blockIdx.x] = shared[0]

每个block输出一个值，包含该block处理的512个原始数据的和
```

## 完整流程图

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  全局内存数据    │    │   数据加载+求和   │    │   共享内存归约   │
│  (N个元素)      │───▶│  每线程处理2个   │───▶│  log2(256)=8轮  │
│                │    │    元素求和      │    │                │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                                      │
                                                      ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │   最终输出      │◀───│  Thread 0写回   │
                       │ (block_num个值)  │    │   shared[0]     │
                       └─────────────────┘    └─────────────────┘
```

## V4版本的优势

1. **减少Block数量**: 每个block处理2倍数据量，block数量减半
2. **提高内存带宽利用率**: 在加载数据时就进行第一次求和
3. **减少同步开销**: 更少的block意味着更少的全局同步点
4. **保持高效的共享内存访问模式**: 仍然避免bank conflict

## 性能特点

- **数据处理量**: 每个block处理 2 × THREAD_PER_BLOCK 个元素
- **内存访问**: 连续访问，cache友好
- **归约轮数**: log₂(THREAD_PER_BLOCK) = 8轮
- **活跃线程递减**: 256 → 128 → 64 → 32 → 16 → 8 → 4 → 2 → 1
